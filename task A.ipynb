{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L5ivJE1pY40f"
      },
      "id": "L5ivJE1pY40f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tutorial_path = \"drive/MyDrive/DM2024-Tutorials/\"\n",
        "dependencies = f'{tutorial_path}dependencies/'"
      ],
      "metadata": {
        "id": "J567JDRUZsJn"
      },
      "id": "J567JDRUZsJn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b0d2bece",
      "metadata": {
        "id": "b0d2bece"
      },
      "source": [
        "<a id=\"0\"></a>\n",
        "\n",
        "# ðŸ“š Tutorial - Rhythmic Structure\n",
        "\n",
        "> **Topics:**\n",
        "> 1. [Introducing Rhythmic Structure](#1)\n",
        "> 2. [Accessing Rhythm](#2)\n",
        "> 3. [Casting Numbers as music21 Scores (Short)](#3)\n",
        "\n",
        "> **By the end of the session you will:**\n",
        "> - âœ… Extract rhythm metrics from xml files using music21\n",
        "> - âœ… Translate sequences of numbers into musical scores using music21\n",
        "> - âœ… Instantiate and perform operations with a new type of rhythmic object in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b8af2a",
      "metadata": {
        "id": "76b8af2a"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "\n",
        "***---***\n",
        "\n",
        "## 1. Introducing Rhythmic Structure\n",
        "###### [back to top](#0)\n",
        "\n",
        "***---***\n",
        "\n",
        "#### ðŸ”µ What?\n",
        "\n",
        "A pared down explanation of rhythm might emphasize:\n",
        "- **duration**: the amount of time a musical event (note, chord, rest) or group of musical events lasts\n",
        "- **onset**: the location in a score or measure at which an event begins, typically measured in terms of beat number\n",
        "- **rest**: the location and balance of moments which do not stimulate the listener (especially in contrast with those that do)\n",
        "\n",
        "The importance of these elements for rhythmic *structure* arises in how they reinforce or disrupt **meter**, or the recurring pattern of musical event timings that drives a musical piece. Meter is specified in part by the time signature, which informs the number of beats per measure, and in part by onsets, durations, and rests, which implicitly assign importance to certain beats on the **metrical grid**:\n",
        "\n",
        "\n",
        "![metrical_grid](https://drive.google.com/uc?export=view&id=18iGMM9nleNfxG48PixLlLtP0Qa84JzH-)\n",
        "\n",
        "\n",
        "In short: rhythmic structure is the scaffolding that defines how often and for how long the listener can expect to hear sounds.\n",
        "\n",
        "#### ðŸŸ¡ How?\n",
        "\n",
        "From the symbolic perspective, composers select the sounded and unsounded durations they desire for their music:   \n",
        "\n",
        "\n",
        "![notes_rests](https://drive.google.com/uc?export=view&id=1pYjeytLogXmT9oh6SN7LAH_EpwWAOd_-)\n",
        "\n",
        "\n",
        "The standard unit of measurement in rhythmic analysis is the <u>beat</u> or, equivalently, the unpitched note corresponding to one beat in the active time signature (eg in the chart above, which lists a 4/4 time signature in the final column header, the durational unit could be the quarter note).\n",
        "\n",
        "Apart from duration, onset, and rest, there are further symbolic aspects (possibly syncopation) and elements of musical performance (rubato) which, though beyond the scope of this introductory notebook, feed into rhythmic structure. All of these intentional rhythmic elements ultimately interact with the listener to bring life to structure.\n",
        "\n",
        "#### ðŸŸ£ Why?\n",
        "\n",
        "We claim rhthmic elements *interact* with the listener because **rhythm exists in the mind.** When we listen to music, we unconsciously learn the timing pattern characteristic to a piece and begin to anticipate musical events based on this learned pattern. This point is not to be missed: to listen to music is to encode all time-based events into a structure that allows us to form expectations. *This* is where rhythmic structure really exists.\n",
        "\n",
        "The great benefit of this process is that musicians can rely it to shape an experience. As listeners come to expect a certain pattern, continued reinforcement feels earned and satisfying while deviations can surprise, delight, or worry. Dance becomes possible. Emotions can be inspired. Stories can be told. What we have is a way to start pulling levers with respect to unconscious human behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb645f8",
      "metadata": {
        "id": "fcb645f8"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid orange;background-color:#F3F3F3;\">\n",
        "    <h3>Structure of today's exercise</h3>\n",
        "In this exercise session, we will introduce computational tools for studying, generating, and comparing rhythms. We will define rhythm, access its symbolic aspects in music21, create onset distribution plots in pursuit of metrical grids, train Markov Chain transition tables for use in sequential generation, apply a set of rhythm grammar rules in hierarchical generation, and implement both feature-based and transformation-based rhythm comparison pipelines.\n",
        "\n",
        "Your task is to fill in missing pieces of code when asked. For each one, you will be given a ðŸŽ¯ `Goal` guidance that explains the steps you need to implement and a ðŸ’» `API` reference with the steps, functions, or classes you can use to achieve each goal.\n",
        "\n",
        "#### Setup\n",
        "All the Python packages you need to install are listed in the next block. This tutorial also assumes you have MuseScore or Finale Notepad installed.\n",
        "\n",
        "#### Data\n",
        "All data is included in the dependencies folder.\n",
        "\n",
        "The Section 2 blue bossa sheet music is available on [MuseScore](https://musescore.com/will_rowsell/blue-bossa-piano)   \n",
        "The Sections 2-3 Mozart piano sonata note transcriptions are from a DCML corpus on [GitHub](https://github.com/DCMLab/mozart_piano_sonatas/tree/main/MS3)   \n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411edcc9",
      "metadata": {
        "id": "411edcc9"
      },
      "source": [
        "#### 1.1 Get set up\n",
        "\n",
        "Create and activate a virtual environment to install the packages in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6452edbb",
      "metadata": {
        "id": "6452edbb"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas pickle seaborn music21 iteration_utilities\n",
        "!pip install git+https://github.com/quadrismegistus/prosodic.git\n",
        "!apt install musescore3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016ed905",
      "metadata": {
        "id": "016ed905"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import music21\n",
        "\n",
        "from fractions import Fraction\n",
        "from collections import defaultdict, Counter\n",
        "from iteration_utilities import deepflatten #flatten nested lists\n",
        "\n",
        "from music21 import midi, note, stream, instrument, meter, key\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef63fda2",
      "metadata": {
        "id": "ef63fda2"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "\n",
        "***---***\n",
        "\n",
        "\n",
        "## 2. Accessing Rhythm\n",
        "\n",
        "***---***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0f0666",
      "metadata": {
        "id": "3c0f0666"
      },
      "source": [
        "Let's move into computational analysis of rhythm. First, let's directly access the elements described above (duration, onset, rest) via music21."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fdea642",
      "metadata": {
        "id": "6fdea642"
      },
      "source": [
        "#### 2.1 Pull in the sample score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f62214d",
      "metadata": {
        "id": "1f62214d"
      },
      "source": [
        "First, pull in our first sample score and have a listen.\n",
        "\n",
        "Appreciate the original version while you can! We're about to take a sledgehammer to it in preparation for rhythm analysis. âš’ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae0eb52",
      "metadata": {
        "id": "9ae0eb52"
      },
      "outputs": [],
      "source": [
        "import music21\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Pull in the score and listen to it a bit\n",
        "sample_score_tmp = music21.converter.parse(f'{dependencies}blue_bossa.mxl')\n",
        "sample_score_tmp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f34ba7d",
      "metadata": {
        "id": "4f34ba7d"
      },
      "source": [
        "A groovy (and famous) little tune. But we don't need all that groove! The next cell uses a function in helpers.py to strip away all the things we really don't need to study rhythm. As with all functions from helpers.py, you are encouraged but not required to take a look at the underlying code, which may be a good reference for your own work in the future."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "def helpers_simplify_score_for_rhythm_analysis(sample_score_tmp):\n",
        "\n",
        "    # remove chord charts, metronome mark, dynamics\n",
        "    for part in sample_score_tmp.parts:\n",
        "        for measure in part.getElementsByClass('Measure'):\n",
        "            for event in measure:\n",
        "                if isinstance(event, music21.harmony.ChordSymbol):\n",
        "                    measure.remove(event)\n",
        "                if isinstance(event, music21.tempo.MetronomeMark):\n",
        "                    measure.remove(event)\n",
        "                if isinstance(event, music21.dynamics.Dynamic):\n",
        "                    measure.remove(event)\n",
        "                if isinstance(event, music21.key.KeySignature):\n",
        "                    measure.remove(event)\n",
        "\n",
        "    # combine voice tracks intra-staff (done by extracted staffs and recombining)\n",
        "    chordified_treble = sample_score_tmp.parts[0].chordify()\n",
        "    chordified_bass = sample_score_tmp.parts[1].chordify()\n",
        "    sample_score = music21.stream.Score()\n",
        "    sample_score.insert(0, chordified_treble)\n",
        "    sample_score.insert(0, chordified_bass)\n",
        "\n",
        "    # cast all single-note chord objects (created by chordify) as note objects so students see more typical music21 score object structure\n",
        "    staff_count = 0\n",
        "    for part in sample_score.parts:\n",
        "        staff_count += 1\n",
        "        for measure in part.getElementsByClass('Measure'):\n",
        "            for event in measure:\n",
        "                if isinstance(event, music21.chord.Chord): # and len(event.notes) == 1:\n",
        "                    note_version = music21.note.Note()\n",
        "                    if staff_count == 1:\n",
        "                        note_version.pitch = music21.pitch.Pitch('C4')\n",
        "                    elif staff_count == 2:\n",
        "                        note_version.pitch = music21.pitch.Pitch('C3')\n",
        "                    note_version.duration = event.duration\n",
        "                    start_found = 0\n",
        "                    continue_found = 0\n",
        "                    none_found = 0\n",
        "                    all_ties_stop = 1\n",
        "                    for nte in event.notes:\n",
        "                        try:\n",
        "                            curr_tie_obj = nte.tie.type\n",
        "                        except:\n",
        "                            curr_tie_obj = None\n",
        "                        if (curr_tie_obj == 'start'):\n",
        "                            start_found = 1\n",
        "                        if (curr_tie_obj == 'continue'):\n",
        "                            continue_found = 1\n",
        "                        if (curr_tie_obj is None):\n",
        "                            none_found = 1\n",
        "                        if (curr_tie_obj != 'stop'):\n",
        "                            all_ties_stop = 0\n",
        "                    if start_found == 1:\n",
        "                        note_version.tie = music21.tie.Tie('start')\n",
        "                    elif none_found == 1:\n",
        "                        note_version.tie = None\n",
        "                    elif all_ties_stop == 1:\n",
        "                        note_version.tie = music21.tie.Tie('stop')\n",
        "                    elif continue_found == 1:\n",
        "                        note_version.tie = music21.tie.Tie('continue')\n",
        "                    note_version.offset = event.offset\n",
        "                    note_version.articulations = event.articulations\n",
        "                    note_version.expressions = event.expressions\n",
        "                    measure.replace(event, note_version)\n",
        "\n",
        "    # fix instances where tie start leads to tie None (should be no tie but previous loop isn't built to observe two consecutive elements)\n",
        "    # the ties are a pain >:\"{\n",
        "    for part in sample_score.parts:\n",
        "        for measure in part.getElementsByClass('Measure'):\n",
        "            for i in range(len(measure)-1):\n",
        "                curr_event = measure[i]\n",
        "                next_event = measure[i+1]\n",
        "                if isinstance(curr_event, music21.note.Note) and isinstance(next_event, music21.note.Note):\n",
        "                    try:\n",
        "                        curr_tie = curr_event.tie.type\n",
        "                    except:\n",
        "                        curr_tie = None\n",
        "                    try:\n",
        "                        next_tie = next_event.tie.type\n",
        "                    except:\n",
        "                        next_tie = None\n",
        "                    if (curr_tie == 'start' or curr_tie == 'continue') and (next_tie == 'start' or next_tie is None):\n",
        "                        measure[i].tie = music21.tie.Tie('stop')\n",
        "                if isinstance(curr_event, music21.note.Rest):\n",
        "                    measure[i].tie = None\n",
        "\n",
        "    # combine tied notes intra-staff\n",
        "    for part in sample_score.parts:\n",
        "        active_tie = 0\n",
        "        for measure in part.getElementsByClass('Measure'):\n",
        "            for i in range(len(measure)):\n",
        "                if isinstance(measure[i], music21.note.Note):\n",
        "                    if measure[i].tie is not None:\n",
        "                        if measure[i].tie.type == \"start\":\n",
        "                            active_tie = 1\n",
        "                        elif measure[i].tie.type == \"stop\" and active_tie == 1:\n",
        "                            active_tie = 0\n",
        "                        elif measure[i].tie.type == \"stop\" and active_tie == 0:\n",
        "                            measure[i].tie = None\n",
        "\n",
        "    treble = sample_score.parts[0]\n",
        "    for measure in treble.getElementsByClass('Measure'):\n",
        "        i = 0\n",
        "        len_measure = len(measure)\n",
        "        while i < len_measure-1:\n",
        "            if isinstance(measure[i], music21.note.Note) and isinstance(measure[i+1], music21.note.Note):\n",
        "                if measure[i].tie is not None and measure[i+1].tie is not None:\n",
        "                    measure[i].duration = music21.duration.Duration(measure[i].duration.quarterLength + measure[i+1].duration.quarterLength)\n",
        "                    if measure[i+1].tie is not None:\n",
        "                        if measure[i+1].tie.type == \"stop\":\n",
        "                            measure[i].tie = None\n",
        "                    measure.remove(measure[i+1])\n",
        "            i += 1\n",
        "            len_measure = len(measure)\n",
        "\n",
        "    bass = sample_score.parts[1]\n",
        "    for measure in bass.getElementsByClass('Measure'):\n",
        "        i = 0\n",
        "        len_measure = len(measure)\n",
        "        while i < len_measure-1:\n",
        "            if isinstance(measure[i], music21.note.Note) and isinstance(measure[i+1], music21.note.Note):\n",
        "                if measure[i].tie is not None and measure[i+1].tie is not None:\n",
        "                    measure[i].duration = music21.duration.Duration(measure[i].duration.quarterLength + measure[i+1].duration.quarterLength)\n",
        "                    if measure[i+1].tie is not None:\n",
        "                        if measure[i+1].tie.type == \"stop\":\n",
        "                            measure[i].tie = None\n",
        "                    measure.remove(measure[i+1])\n",
        "            i += 1\n",
        "            len_measure = len(measure)\n",
        "\n",
        "    sample_score = music21.stream.Score()\n",
        "    sample_score.insert(0, treble)\n",
        "    sample_score.insert(0, bass)\n",
        "\n",
        "    # deciding against repeat expands for now due to issues with measure number\n",
        "\n",
        "    return sample_score\n"
      ],
      "metadata": {
        "id": "v3lb8ikcb2t0"
      },
      "id": "v3lb8ikcb2t0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0657539d",
      "metadata": {
        "id": "0657539d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"dependencies\")\n",
        "\n",
        "# Strip out unnecessary elements\n",
        "sample_score = helpers_simplify_score_for_rhythm_analysis(sample_score_tmp)\n",
        "sample_score.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3303093",
      "metadata": {
        "id": "c3303093"
      },
      "source": [
        "\n",
        "Key signature, chord vs. note, separate voice tracks intra-staff, chord charts, dynamics, **pitch** - these are all <u>irrelevant</u>! It may not be as exciting to listen to (see for yourself), but it should be easier to deal with visually and in music21.\n",
        "\n",
        "Let's print out the rhythm elements described in the theory above for the first two measures of our sample score. The loop below may be offputting, but make sure you understand it. It's showing you how to access event-level information from a score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b3fc30",
      "metadata": {
        "id": "36b3fc30"
      },
      "outputs": [],
      "source": [
        "import music21\n",
        "print(\"Data rows will be of the form:\\n<type> <onset_in_measure> <duration> <tie> \\n\")\n",
        "print(\"Time Signature: \" + sample_score.flat.getElementsByClass('TimeSignature')[0].ratioString + \"\\n\")\n",
        "\n",
        "for staff in sample_score.parts:\n",
        "    staff_name = staff.elements[1].clef.name\n",
        "    for measure in staff.getElementsByClass('Measure'):\n",
        "        if measure.measureNumber <= 2 and measure.measureNumber > 0:\n",
        "            print(staff_name + \" measure \" + str(measure.measureNumber))\n",
        "            for event in measure.recurse():\n",
        "                label = \"\"\n",
        "                if isinstance(event, music21.note.Note):\n",
        "                    label = \"sounded\"\n",
        "                if isinstance(event, music21.note.Rest):\n",
        "                    label = \"unsounded\"\n",
        "                try:\n",
        "                    tie_info = \"tie_\" + event.tie.type\n",
        "                except:\n",
        "                    tie_info = \"\"\n",
        "                if label != \"\":\n",
        "                    print(label, event.offset, event.duration.quarterLength, tie_info)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add6460d",
      "metadata": {
        "id": "add6460d"
      },
      "source": [
        "![bossa_m1m2](https://drive.google.com/uc?export=view&id=1d1gTs6QYCXHLwccPBl8VFmoKK0M4YGoH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379d52c1",
      "metadata": {
        "id": "379d52c1"
      },
      "source": [
        "To address the rows:\n",
        "- 'Sounded' or 'unsounded' is self explanatory\n",
        "- The second datapoint in each row refers to **onset**: the (0-indexed) beat at which the note or rest begins. Hence, 3.5 means an event begins halfway through the *fourth* (and final) beat.\n",
        "- The third datapoint in each row corresponds to **duration**, and is measured in quarter notes via music21's quarterLength attribute. In this case, this number equivalently refers to number of beats - but **only** because the time signature has a 4 at the bottom. **quarterLength will not always refer to beat count!**\n",
        "- Tie information is occasionally a fourth datapoint. That a note <u>shares an onset</u> with a previous note is relevant to rhythm analysis.\n",
        "\n",
        "Time for *you* to get involved. Would you please throw the data above (+ a bit more) into a dataframe? It'd be very kind of you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87de715d",
      "metadata": {
        "id": "87de715d"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Cast the rhythm data in the sample score as a pandas dataframe. The dataframe should have one note or rest per row, with the following columns:\n",
        "  \n",
        "  - *staff*: \"treble\" or \"bass\"\n",
        "  - *measure_number*: 1, 2, 3, ...\n",
        "  - *event_type*: \"sounded\" or \"unsounded\"\n",
        "  - *onset_in_measure* (float): [0,4)\n",
        "  - ***onset_in_score*** (float): [0, âˆž)\n",
        "  - *duration* (float): (0,4]\n",
        "  - *tie_info* (string): \"tie_start\", \"tie_continue\", \"tie_stop\", or nothing\n",
        "\n",
        "  Please do not 'unfold' repeat bars. This tutorial was designed without such unfolding. For your own reference, music21 has a method called expandRepeats() that can help with this.<br>\n",
        "\n",
        "- ðŸ’»  **API:**  The code above extracts everything you need except *onset_in_score*. You need to (1) create *onset_in_score* and (2) actually place the data into a pandas dataframe.\n",
        "\n",
        "  - *onset_in_score* can be a simple multiplication composed of measure number and *onset_in_measure*\n",
        "  - There are many ways to create a pandas dataframe. One method would be to maintain a list of 7-element tuples and later convert the list with proper column names.\n",
        "\n",
        "  Hint: start with the code above.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa3761a",
      "metadata": {
        "id": "efa3761a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "## STUDENT SECTION - ##\n",
        "rhythm_data_list = []\n",
        "for clef in sample_score.parts:\n",
        "    global_onset = 0\n",
        "    clef_name = clef.elements[1].clef.name\n",
        "    for measure in clef.getElementsByClass('Measure'):\n",
        "        for event in measure.recurse():\n",
        "            label = \"\"\n",
        "            if isinstance(event, music21.note.Note):\n",
        "                label = \"sounded\"\n",
        "            if isinstance(event, music21.note.Rest):\n",
        "                label = \"unsounded\"\n",
        "            try:\n",
        "                tie_info = \"tie_\" + event.tie.type\n",
        "            except:\n",
        "                tie_info = \"\"\n",
        "            if label != \"\":\n",
        "                global_onset = ((measure.measureNumber-1) * 4) + event.offset\n",
        "                rhythm_data_list.append((clef_name, measure.measureNumber, label, event.offset, global_onset, event.duration.quarterLength, tie_info))\n",
        "rhythm_data_df = pd.DataFrame(rhythm_data_list, columns=['staff', 'measure_number', 'event_type', 'onset_in_measure', 'onset_in_score', 'duration', 'tie_info'])\n",
        "## END STUDENT SECTION ##\n",
        "\n",
        "# Check output with reference photo\n",
        "pd.concat([rhythm_data_df.head(11), rhythm_data_df.tail(4)], axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8ccb90",
      "metadata": {
        "id": "3c8ccb90"
      },
      "source": [
        "\n",
        "\n",
        "![bossa_data](https://drive.google.com/uc?export=view&id=1y85E7GnNF7Mu6K6tSVWwWnAw7pDsL2q2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c7c97e",
      "metadata": {
        "id": "13c7c97e"
      },
      "source": [
        "Great! Now we have access to rhythm-informing datapoints in a nice clean data structure.\n",
        "\n",
        "But.. how do we get a handle on what's really happening with our rhythm?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a491c9",
      "metadata": {
        "id": "21a491c9"
      },
      "source": [
        "#### 2.2 Distribution of Sounded Event Onsets, 4-Beat Range\n",
        "\n",
        "If we're curious about the metrical grid specifically, one option is to study the onset locations of sounded events in our score. Let's do this now.\n",
        "\n",
        "We'll start with the intra-measure onset distribution of sounded events **along the bass staff only**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c0e14e",
      "metadata": {
        "id": "a4c0e14e"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Complete *extract_onset_in_measure()*, which returns a pandas series or Python list containing *onset_in_measure* values for all <u>**bass-staff sounded events**</u>\n",
        "\n",
        "- ðŸ’»  **API:** Filter the *subset* dataframe below for sounded events and extract their *onset_in_measure* values.\n",
        "    - Be sure to remove notes that are tied to previous notes! We're interested in onsets here, so a note tied to a previous note has already 'started' its sound, so to speak.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def helpers_onset_dist(onsets, measure_count = 4):\n",
        "\n",
        "    if type(onsets) is not list:\n",
        "        onsets = onsets.tolist()\n",
        "\n",
        "    beat_locations = [i * 0.5 for i in range(10 * measure_count + 1)]\n",
        "    beat_frequencies = []\n",
        "    for loc in beat_locations:\n",
        "        beat_frequencies.append((onsets.count(loc) / len(onsets)))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(beat_locations, beat_frequencies, color='blue')\n",
        "    if measure_count == 4:\n",
        "        ax.set_xlabel('Onset in Measure')\n",
        "    if measure_count == 8:\n",
        "        ax.set_xlabel('Onset in Two-Measure Sequence')\n",
        "        ax.axvline(4, linestyle = '--')\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_title('Relative Frequency of Onset Locations')\n",
        "    ax.set_xlim(0, measure_count)\n",
        "    ax.set_ylim(0, .5)\n"
      ],
      "metadata": {
        "id": "pgiCy-zVcZEE"
      },
      "id": "pgiCy-zVcZEE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b63a8ac",
      "metadata": {
        "id": "5b63a8ac"
      },
      "outputs": [],
      "source": [
        "subset = rhythm_data_df[rhythm_data_df['staff'] == 'bass']\n",
        "\n",
        "def extract_onset_in_measure(subset):\n",
        "    ## STUDENT SECTION -  ##\n",
        "    return subset[(subset['event_type'] == \"sounded\") & (subset['tie_info'] != \"tie_stop\")]['onset_in_measure']\n",
        "    ## END STUDENT SECTION ##\n",
        "\n",
        "onsets_bass = extract_onset_in_measure(subset)\n",
        "helpers_onset_dist(onsets_bass, 4) # graphing function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b407f1",
      "metadata": {
        "id": "72b407f1"
      },
      "source": [
        "There are two clear maxima at the first and third beats in the measure (recall 0-indexing). Apart from the increase ahead of beat 3, sounded events infrequently occur at other onset locations.\n",
        "\n",
        "If we wanted to draw a metrical grid for this score's **bass clef only**, and we restricted ourselves to drawing beat emphasis circles only on the beat level (as opposed to the sub-beat level), the graph above recommends the grid below.\n",
        "\n",
        "\n",
        "\n",
        "![bossa_grid_bass](https://drive.google.com/uc?export=view&id=1YNVB0fWGSZ2FJPOZElqaTBd0csBIiRCC)\n",
        "\n",
        "\n",
        "\n",
        "As you can see, the score also visually recommends this austere, intuitive rhythmic pattern. But does this grid make sense to your ear? Have a listen to the bass staff alone with the next block. Clap along to the beat. Ensure you make it past the first four measures ðŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2ae2f1",
      "metadata": {
        "id": "fc2ae2f1"
      },
      "outputs": [],
      "source": [
        "sample_score.parts[1].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db014325",
      "metadata": {
        "id": "db014325"
      },
      "source": [
        "Sounds reasonable! The first and third beats consistently accompany bass measures in the piece, so much so that occasional ornamentation doesn't disrupt our mental encoding of the metrical system depicted above.\n",
        "\n",
        "Let's isolate the treble staff to perform the same analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba60c296",
      "metadata": {
        "id": "ba60c296"
      },
      "outputs": [],
      "source": [
        "subset = rhythm_data_df[rhythm_data_df['staff'] == 'treble']\n",
        "onsets_treble = extract_onset_in_measure(subset)\n",
        "helpers_onset_dist(onsets_treble, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6366122e",
      "metadata": {
        "id": "6366122e"
      },
      "source": [
        "Hmm... it doesn't appear to be as simple. Once again, beat 1 has a high share in the distribution. But the local peak at beat 3 has been 'pulled forward' to beat 2.5, and the global maximum occurs halfway into the final beat. This is strange.\n",
        "\n",
        "#### 2.3 Distribution of Sounded Event Onsets, 8-Beat Range\n",
        "\n",
        "Something we haven't accounted for yet is the possibility that the rhythmic structure is operating at a timescale longer than one measure. Go ahead and pass in onsets not as intra-measure onsets but intra-measure-pair onsets (that is, onsets relative to an 8-beat range rather than 4-beat range)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e8302e",
      "metadata": {
        "id": "11e8302e"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Complete *extract_onset_in_measure_pair()*, which returns a pandas series or list containing onset values for all <u>**treble-staff sounded events**</u>. Unlike before, these onset values should range between 0 and 7.5; that is, our new timescale is 8 beats rather than 4.\n",
        "\n",
        "- ðŸ’»  **API:** Filter the *subset* dataframe below for sounded events, extract their *onset_in_score* values, and perform a modulus operation on the extracted values. Once again, remove events tied to previous events.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63348470",
      "metadata": {
        "id": "63348470"
      },
      "outputs": [],
      "source": [
        "subset = rhythm_data_df[rhythm_data_df['staff'] == 'treble']\n",
        "\n",
        "def extract_onset_in_measure_pair(subset):\n",
        "    ## STUDENT SECTION -  ##\n",
        "    return subset[(subset['event_type'] == \"sounded\") & (subset['tie_info'] != \"tie_stop\")]['onset_in_score']%8\n",
        "    ## END STUDENT SECTION ##\n",
        "\n",
        "onsets_8beats = extract_onset_in_measure_pair(subset)\n",
        "helpers_onset_dist(onsets_8beats, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cad9e6",
      "metadata": {
        "id": "43cad9e6"
      },
      "source": [
        "There appears to be quite a weird distortion occurring at the turn of the measure. Let's overlay the two measure-length beat sequences to better visualize.\n",
        "\n",
        "Notice that our 8-beat sequence can equivalently be thought of as a view of the onset distribution for odd (beats 1-4) and even (beats 5 - 8) measures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def helpers_onset_dist_overlap_measure_pair(onsets):\n",
        "\n",
        "    if type(onsets) is not list:\n",
        "        onsets = onsets.tolist()\n",
        "\n",
        "    beat_locations = [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
        "\n",
        "    m1_onsets = [o for o in onsets if o < 4]\n",
        "    m2_onsets = [o-4 for o in onsets if o >= 4]\n",
        "\n",
        "    m1_frequencies = []\n",
        "    m2_frequencies = []\n",
        "    for loc in beat_locations:\n",
        "        m1_frequencies.append((m1_onsets.count(loc) / len(onsets)))\n",
        "        m2_frequencies.append((m2_onsets.count(loc) / len(onsets)))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(beat_locations, m1_frequencies, color='blue')\n",
        "    ax.plot(beat_locations, m2_frequencies, color='red')\n",
        "    ax.set_xlabel('Onset in Measure')\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_title('Relative Frequency of Onset Locations')\n",
        "    ax.set_xlim(0, 4)\n",
        "    ax.set_ylim(0, .5)\n",
        "    ax.legend([\"Odd Measure\", \"Even Measure\"])\n"
      ],
      "metadata": {
        "id": "8Pxm40xNckC8"
      },
      "id": "8Pxm40xNckC8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17badff",
      "metadata": {
        "id": "a17badff"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"dependencies\")\n",
        "\n",
        "helpers_onset_dist_overlap_measure_pair(onsets_8beats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f72a470",
      "metadata": {
        "id": "2f72a470"
      },
      "source": [
        "Two of the most striking disparities occur at the turn of the measure (that is, both the beginning of end of the four beats above). Odd-numbered measures outpace even-numbered measure onsets at the start of the initial beat and end of the final beat. Curiously, odd-numbered measures lack onsets at the beginning of beat four but still have a marked share at the final half beat. Even-numbered measures, meanwhile, show quite a steady distribution of onset locations - except on beat 3, where they have none (once again: don't forget 0-indexing!). Even-numbered measures also conspicuously lack an outsized share of onsets at the first beat.\n",
        "\n",
        "What exactly is going on? Far and away the most important dynamic at play is the 'pulling forward' of notes from beat 5 (the start of even measures) to halfway through beat 4 (the end of odd measures). This is what's occurring in the red circles below, where ties pull the onset for sounded events forward, de-emphasizing in the even measures what would typically be the strongest beat. The green and blue circles, meanwhile, demonstrate the greater variety seen at the end of even measures relative to odd measures, which is responsible for the steady final-beat distribution among even measures.\n",
        "\n",
        "Beneath the eight pictured measures you will find a metrical grid **that is based on the onset distributions above**. Here, we allow sub-beat onsets, creating a more granular grid.\n",
        "\n",
        "![bossa_grid](https://drive.google.com/uc?export=view&id=11gECaHkXpzVbRJ8uE4a0nfwW7VI4F6ce)\n",
        "\n",
        "Does the metrical grid make sense to your ear? Have a listen below.\n",
        "\n",
        "\n",
        "Clave pattern. Show histogram over metrical grid.\n",
        "Tresillo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c1001ef",
      "metadata": {
        "id": "4c1001ef"
      },
      "outputs": [],
      "source": [
        "sample_score.parts[0].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfd6e62",
      "metadata": {
        "id": "dbfd6e62"
      },
      "source": [
        "Let's be honest: the metrical grid is a bit less intuitive relative to the one we drew for the bass staff above. Certainly we get the sense that odd measures are more important than even measures in each consecutive 8-beat sequence, and maybe we can identify the beat strength pattern in odd measures now that we've heard blue bossa's main motif multiple times. But this level of granularity is hard to ascertain with the ear:\n",
        "\n",
        "![bossa_treble_aust](https://drive.google.com/uc?export=view&id=1Ah3ZM0mXMGLdhvtiWmZlOQKJKCZK0GAW)\n",
        "\n",
        "\n",
        "\n",
        "There's two reasons behind this. First, ornamentation can crowd our onset distribution with onsets that are technically present but musically less significant. There may not be a simple heuristic for determining which onsets are more important than others and, indeed, melody information can inform this. For now, we will take the rather easy and blunt step of removing musical events less than a beat in length in the hopes that sounded events with longer durations really punctuate the beats that matter.\n",
        "\n",
        "Second, blue bossa's rhythmic grammar changes a bit halfway through the treble staff. Take a look at the green circles in the photo below. These letter indicate sections of the score, ways to segment blue bossa based on measures that follow similar patterns. A, B1, and C are rhythmically similar save for a bit of ornamentation (listen again with the block above to confirm). B2 is an entirely different beast. Meanwhile, our analysis is finding the average onset distribution using 16 measures labeled 'B2' **in combination with** 12 measures labeled A, B1, or C, ensuring our analysis 'straddles' a structural change in rhythmic pattern (of course, this so-called structural change it is not as severe as it would be if we were changing score or genre). For this reason, we'll limit our distribution to measures 5 - 16 (segments A, B1, and C) below.\n",
        "\n",
        "\n",
        "![bossa_treble_seg](https://drive.google.com/uc?export=view&id=11OcUScVnIyboslSrpmYY84ZDHO_Sxm0b)\n",
        "\n",
        "\n",
        "(A third reason is that we're studying the bass and treble clefs separately, but listeners only hear them played together. It is not exactly appropriate to compartmentalize the sounds contributing to rhythm like this. We do so simply so we can use the same piece to explore rhythmic variety. We will not address this issue here - but you will notice that in other sections, we stick with monophonic.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915c069d",
      "metadata": {
        "id": "915c069d"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Complete *new_extract_onset_in_measure_pair()*, which should be the same as *extract_onset_in_measure_pair()* except now only sounded events longer that one beat (or sounded events tied to future events, regardless of duration) are kept\n",
        "\n",
        "- ðŸ’»  **API:** Augment your *extract_onset_in_measure_pair()* code with new filters on *duration* and *tie*\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c058e8",
      "metadata": {
        "id": "01c058e8"
      },
      "outputs": [],
      "source": [
        "subset = rhythm_data_df[(rhythm_data_df['staff'] == 'treble') & ((rhythm_data_df['measure_number'] >= 5) & (rhythm_data_df['measure_number'] < 17))]\n",
        "\n",
        "def new_extract_onset_in_measure_pair(subset):\n",
        "    ## STUDENT SECTION -  ##\n",
        "    return subset[(subset['event_type'] == \"sounded\") & (subset['tie_info'] != \"tie_stop\") & ((subset['duration'] >= 1) | (subset['tie_info'] == \"tie_start\"))]['onset_in_score']%8\n",
        "    ## END STUDENT SECTION ##\n",
        "\n",
        "onsets_8beats = new_extract_onset_in_measure_pair(subset)\n",
        "helpers_onset_dist(onsets_8beats, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241aedbb",
      "metadata": {
        "id": "241aedbb"
      },
      "source": [
        "This is perhaps more intuitive as a guidepost for important beats. Instead of creating yet another metrical grid, let's take this and lay it over the 'normal' onset distribution associated with 4/4 time signature. By referencing a baseline, we'll develop an understanding of the rhythmic eccentricities of our piece.\n",
        "\n",
        "Below, we import a corpus of Mozart Piano Sonatas (available as a DCML corpus [here](https://github.com/DCMLab/mozart_piano_sonatas/tree/main/MS3)), keep only those pieces with 4/4 meter, keep only those sounded events longer than one beat, convert onsets to their position in successive 8-beat sequences, and visualize the onset distribution alongside the distribution you create above. **Note our use of sns.displot(), a seaborn method which simplifies the code for creating our desired chart.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7771c8",
      "metadata": {
        "id": "5f7771c8"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Function for converting onsets to 8-beat scale rather than 4-beat scale\n",
        "def eight_beatize(event):\n",
        "    if event['mn'] % 2 == 0:\n",
        "        return event['onset'] + 4\n",
        "    else:\n",
        "        return event['onset']\n",
        "\n",
        "# Read in Mozart Sonatas\n",
        "folder = f'{dependencies}/Mozart Piano Sonatas'\n",
        "MozartSonatas = ['/'.join([folder,piece]) for piece in os.listdir(folder) if '.tsv' in piece]\n",
        "corpus = pd.concat(map(pd.read_table, MozartSonatas))\n",
        "corpus['duration'] = corpus['duration'].apply(Fraction).astype(float)\n",
        "\n",
        "# Retain only those scores with 4/4 meter, exclude grace notes, exclude notes that last less tahn 1 beat (one quarter note)\n",
        "corpus_4_4 = corpus[(corpus.timesig == '4/4') & corpus.gracenote.isna() & (corpus.duration >= .25)].reset_index()\n",
        "\n",
        "# Cast onsets to position in 8-beat sequences\n",
        "corpus_4_4['onset'] = corpus_4_4['onset'].apply(Fraction).astype(float) * 4\n",
        "corpus_4_4['onset'] = corpus_4_4.apply(eight_beatize, axis = 1)\n",
        "\n",
        "# Combine mozart onsets and blue bossa onsets to permit overlaid density plot\n",
        "onsets = pd.concat([corpus_4_4['onset'], onsets_8beats], axis=0)\n",
        "origin = ['mozart sonatas'] * len(corpus_4_4['onset']) + ['blue bossa'] * len(onsets_8beats)\n",
        "onsets = pd.DataFrame({'onsets': onsets, 'origin': origin})\n",
        "\n",
        "# Use seaborn method to overlay kernel density plots. Note the simplicity even with series of vastly different observation counts :)\n",
        "sns.displot(onsets, x='onsets', hue = 'origin', kind = 'kde', bw_adjust = 0.2, common_norm = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ae3e6a",
      "metadata": {
        "id": "13ae3e6a"
      },
      "source": [
        "The blue line shows us high-duration onset locations in Mozart sonatas. The orange line shows us high-duration onset locations in blue bossa's **treble clef only**.\n",
        "\n",
        "The first of the two highest peaks in blue bossa is precisely where it \"should\" be (if Mozart is the baseline). The second is shifted one half-beat to the left, indicating an oft-syncopated sounded event as odd-numbered measures move into even-numbered measures. Apart from a bit more possible syncopation ahead of the seventh beat, other blue bossa local maxima overlap with 'normal' peaks, partially if not fully. Beats 5-8 clearly receive less sounded events in blue bossa than beats 1-4, despite a Mozart proclivity for sustained presence throughout. Finally, some 'normal' peaks are missing in our sample piece: beats 2 and 7 (0-indexing!). These observations collectively form one perspective on the character of blue bossa's rhythm - though they pay much more attention to onset than they do duration and rest (which is also true of this entire tutorial section).\n",
        "\n",
        "So far, we've explored a way to distill a score into onsets for sounded events, visualized these onsets, and considered how an 8-beat scale (vs. 4-beat scale), a filter on event durations, a filter on measures analyzed, and a classical genre baseline can all flesh out our understanding of rhythm. We have directly translated sounded event durations and onsets into metrical grids. We have seen how structural changes in a piece can complicate conclusions about what sort of rhythm characterizes an entire piece. But this has all been exploratory! While we can (hopefully) speak a bit about our piece's rhythmic behavior in a qualitative, general way, we'd need to do more work to create a rhythm ourselves, much less a rhythm that can mimic our sample piece in a way that feels human and respects blue bossa's time-based grammar.\n",
        "\n",
        "The work above was intended as an introduction to visualizing rhythm, a way to get you thinking about the different variables involved in the recipe while also showing you how to handle them in Python. There are many more ways to quantify and visualize rhythm. A self-similarity matrix of onset locations, durations, or rest presence, for example. A set of extracted features, such as inter-onset interval or average duration. The list goes on.\n",
        "\n",
        "We will explore further ways to represent rhythm when we get to comparison. For now, let's move our efforts from observing rhythm to actually generating it ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96742f65",
      "metadata": {
        "id": "96742f65"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "\n",
        "***---***\n",
        "\n",
        "\n",
        "## 3. Casting Numbers as music21 Scores (Short)\n",
        "###\n",
        "###### [back to top](#0)\n",
        "\n",
        "***---***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0267a987",
      "metadata": {
        "id": "0267a987"
      },
      "source": [
        "#### 3.1 Introducing the generative Python functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd0b51a",
      "metadata": {
        "id": "cfd0b51a"
      },
      "source": [
        "Before we generate with rhythm as our explicit target, let's take a look at the general score-creation function we provide in helpers.py: **helpers_rhythm_from_sequence()**\n",
        "\n",
        "We use it below for a trivially simple example."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "import fractions\n",
        "from fractions import Fraction\n",
        "\n",
        "UNIT=1\n",
        "\n",
        "\n",
        "def helpers_sounded_event(duration, pitch = \"C4\"):\n",
        "    #Create Note object for percussion hits (default pitch is C4)\n",
        "    return music21.note.Note(pitch, quarterLength = duration*(4*UNIT))\n",
        "\n",
        "def helpers_append_event(duration, original_stream, rest = False, pitch = 'C4'):\n",
        "    #Returns a new_stream obtained by appending a rhythmical event or a rest of given duration to the original_stream\n",
        "    new_stream = original_stream\n",
        "    if rest:\n",
        "        new_stream.append(music21.note.Rest(quarterLength = duration*(4*UNIT)))\n",
        "    else:\n",
        "        new_stream.append(helpers_sounded_event(duration, pitch))\n",
        "    return new_stream\n",
        "\n",
        "def helpers_create_stream(time_sig = None, instru = music21.instrument.Woodblock()):\n",
        "    #Initialize a percussion stream with Woodblock timbre\n",
        "    #If time signature is None, no measure splits\n",
        "    if time_sig == None:\n",
        "        instruPart = music21.stream.Measure()\n",
        "    else:\n",
        "        instruPart = music21.stream.Stream()\n",
        "        instruPart.timeSignature = music21.meter.TimeSignature(time_sig)\n",
        "\n",
        "    instruPart.insert(0, instru) #assign woodblock timbre\n",
        "    return instruPart\n",
        "\n",
        "def helpers_rhythm_from_sequence(durations, time_sig = None, pitch = 'C4', rhythm=None, instru = music21.instrument.Woodblock()):\n",
        "    #Generate rhythmic stream from a list of durations. Rests are indicated by specifying a duration as a string\n",
        "    if rhythm is None:\n",
        "        # pass an existing stream 'rhythm' to append the durations, otherwise a new one will be created\n",
        "        rhythm = helpers_create_stream(time_sig = time_sig, instru = instru)\n",
        "    for dur in durations:\n",
        "        is_rest = False\n",
        "        if dur != 0:\n",
        "            if isinstance(dur, str):\n",
        "                #if duration is given as a string, interpret as rest and turn string into a numerical value\n",
        "                is_rest = True\n",
        "                try:\n",
        "                    dur = float(dur)\n",
        "                except:\n",
        "                    dur = fractions.Fraction(dur)\n",
        "\n",
        "            rhythm = helpers_append_event(dur, rhythm, rest = is_rest, pitch = pitch)\n",
        "    return rhythm"
      ],
      "metadata": {
        "id": "vFwcfiOHcwKc"
      },
      "id": "vFwcfiOHcwKc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707c669b",
      "metadata": {
        "id": "707c669b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"dependencies\")\n",
        "\n",
        "helpers_rhythm_from_sequence([1/2, .25, 1/8, 1/16, 1/32, 1/64, 1, 1.5]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8972c2b2",
      "metadata": {
        "id": "8972c2b2"
      },
      "source": [
        "Voila. You pass in a sequence of durations and receive a score in return. The durations can be written in either fraction or decimal notation. The only must is that they correspond to a note duration recognized by music21 (anything 'legal' in basic music theory: eighth note, sixteenth note, half note, etc), with the number 1 corresponding to a whole note.\n",
        "\n",
        "If you do not specify a time signature, *helpers_rhythm_from_sequence()* simply extends the measure for as long as is necessary and modifies the time signature accordingly. This doesn't always produce an ideal time signature though... it's not often you find music composed in 223/64 time.\n",
        "\n",
        "Let's see what happens when we define a time signature and make a few other input alterations to boot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a53398",
      "metadata": {
        "id": "22a53398"
      },
      "outputs": [],
      "source": [
        "helpers_rhythm_from_sequence([1/2, .25, 1/8, '1/16', 1/32, 1/64, '1', 1.5], time_sig ='4/4', instru = music21.instrument.Harp()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d9e2ae",
      "metadata": {
        "id": "30d9e2ae"
      },
      "source": [
        "A few important things just happened.\n",
        "- By defining the time signature, we forced certain musical events to spread across multiple measures. Take a look at the final input, denoted by 1.5 in our input sequence. It's a single 6-beat sound stretched across three measures.\n",
        "- We've included rests. Simply cast a duration as a string upon input to create a rest. As with notes, rests are automaticaly split across measures when the time signature demands it.\n",
        "- We customized our instrument (this is audible when we play the score). Woodblocks (the default) are perfect for hearing onsets but less perfect for hearing durations.\n",
        "\n",
        "Time to prove you understand the function we will use to generate groovy tunes!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0e9d5e",
      "metadata": {
        "id": "de0e9d5e"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Recreate the following rhythm:\n",
        "\n",
        "![humpty](https://drive.google.com/uc?export=view&id=18VBW9laju1zyBL8qMIYbywlrZmqzyoz8)\n",
        "\n",
        "\n",
        "\n",
        "- ðŸ’»  **API:** Use *helpers_rhythm_from_sequence()* with the appropriate time signature and list of durations. Use an instrument of your choice (check out the full list on the right side of [this](https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html) webpage)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacda5f8",
      "metadata": {
        "id": "bacda5f8"
      },
      "outputs": [],
      "source": [
        "## STUDENT SECTION - ##\n",
        "recreated_rhythm = helpers_rhythm_from_sequence([], time_sig ='6/8', instru = music21.instrument.Lute())\n",
        "## END STUDENT SECTION ##\n",
        "recreated_rhythm.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6751837",
      "metadata": {
        "id": "e6751837"
      },
      "source": [
        "Humpty Dumpty sat on a wall...\n",
        "\n",
        "Before we move to our first generative approach, we will demonstrate a custom-built function for combining measures: *helpers_combine_scores()*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "import copy\n",
        "\n",
        "def helpers_combine_scores(score_base, oth_score_list):\n",
        "    if type(oth_score_list) != list:\n",
        "        oth_score_list = [oth_score_list]\n",
        "    new_score = copy.deepcopy(score_base)\n",
        "    for oth_score in oth_score_list:\n",
        "        for event in oth_score.recurse():\n",
        "            if isinstance(event, music21.note.Note) or isinstance(event, music21.note.Rest):\n",
        "                new_event = copy.deepcopy(event)\n",
        "                new_score.append(new_event)\n",
        "    return new_score"
      ],
      "metadata": {
        "id": "afSfuBMAc7GG"
      },
      "id": "afSfuBMAc7GG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27aee18",
      "metadata": {
        "id": "a27aee18"
      },
      "outputs": [],
      "source": [
        "recreated_rhythm_2 = helpers_rhythm_from_sequence([1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/4, 1/8], time_sig = '6/8', instru = music21.instrument.Lute())\n",
        "combined_scores = helpers_combine_scores(recreated_rhythm, recreated_rhythm_2)\n",
        "\n",
        "recreated_rhythm.show()\n",
        "recreated_rhythm_2.show()\n",
        "combined_scores.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a0e592",
      "metadata": {
        "id": "60a0e592"
      },
      "source": [
        "This function simply appends each event from the latter score to the former in sequence. The final time signature therefore comes from the first score. Appending begins at the precise end of the first score regardless of where in a measure that ending occurs. In other words, our combined result above respects the second measure break only because our first score fills exactly four measures.\n",
        "\n",
        "Now that you have access to functions which cast duration sequences as music21 scores, we need only develop intelligent rules for crafting duration sequences. And if you're worried our duration-only focus is too parochial, fear not: we are inadvertently controlling onsets and underlying metrical grid - the other core elements of rhythm - by choosing the order and amount of durations in addition to the durations themselves.\n",
        "\n",
        "To get the juices flowing, let's quickly incoporate our primary sequence-to-score function in a real application that involves sampling from probability distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17960995",
      "metadata": {
        "id": "17960995"
      },
      "source": [
        "#### 3.2 Using the generative Python functions: onset-based duration distribution application"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59470d6f",
      "metadata": {
        "id": "59470d6f"
      },
      "source": [
        "Recall our Mozart Sonata corpus, imported again below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7b5ea3",
      "metadata": {
        "id": "9a7b5ea3"
      },
      "outputs": [],
      "source": [
        "folder = f'{dependencies}/Mozart Piano Sonatas'\n",
        "MozartSonatas = ['/'.join([folder,piece]) for piece in os.listdir(folder) if '.tsv' in piece]\n",
        "corpus = pd.concat(map(pd.read_table, MozartSonatas))\n",
        "corpus['duration'] = corpus['duration'].apply(Fraction).astype(float) * 4\n",
        "corpus_4_4 = corpus[(corpus.timesig == '4/4') & corpus.gracenote.isna()].reset_index()\n",
        "corpus_4_4['onset'] = corpus_4_4['onset'].apply(Fraction).astype(float) * 4\n",
        "corpus_4_4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf7114f",
      "metadata": {
        "id": "acf7114f"
      },
      "source": [
        "Recall further the onset distribution charts we created. Let's recreate the distribution for Mozart sonatas, this time **without** differentiating between even and odd measures. Onsets that appear less than 10 times across all ~27K musical events are excluded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b983b405",
      "metadata": {
        "id": "b983b405"
      },
      "outputs": [],
      "source": [
        "onset_tab = corpus_4_4['onset'].value_counts()\n",
        "onsets_sans_rare_idx = onset_tab[onset_tab >= 10].index\n",
        "onsets_sans_rare = corpus_4_4[corpus_4_4['onset'].isin(onsets_sans_rare_idx)]\n",
        "sns.displot(onsets_sans_rare, x='onset', kind = 'kde', bw_adjust = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c41a465",
      "metadata": {
        "id": "3c41a465"
      },
      "source": [
        "It turns out we can use this analysis (+ a bit more) to create generative rules. Specifically, we can:\n",
        "\n",
        "1. Select an initial onset location using a distribution of Mozart's onset locations\n",
        "2. Calculate the distribution of note durations seen at each onset location, and\n",
        "3. Use the distributions in step 2 to repeately select a duration, arrive at a new onset, select a duration, arrive at a new onset, etc.\n",
        "\n",
        "Visually, this is the process we're describing:\n",
        "\n",
        "![gen](https://drive.google.com/uc?export=view&id=1Me-G0Ztvf6V5tnh06YMX_Ty4zZdpR_Ax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05ebbae",
      "metadata": {
        "id": "b05ebbae"
      },
      "source": [
        "We must first calculate (1) a distribution of all onsets for the choice of initial onset and (2) a distribution of durations at each onset. These can be handled in one line each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60374e9b",
      "metadata": {
        "id": "60374e9b"
      },
      "outputs": [],
      "source": [
        "onset_distribution = corpus_4_4.onset.value_counts(normalize = True)\n",
        "duration_distribution_by_onset = corpus_4_4.groupby('onset').duration.value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b0175c",
      "metadata": {
        "id": "c9b0175c"
      },
      "source": [
        "*duration_distribution_by_onset* reflects Mozart's favorite and least favorite duration choices at any given onset. It contains, for each onset, the data necessary to create the red-lined charts above.\n",
        "\n",
        "Now we just need to implement the rest of the steps in the picture above. Why don't you help us out with this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a13ef2",
      "metadata": {
        "id": "d7a13ef2"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Complete the function *select_initial_onset()*. The function should select and return an onset from *onset_distribution* by sampling from the distribution.\n",
        "\n",
        "- ðŸ’»  **API:** Investigate the structure of onset_distribution and then check out np.random.choice().\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6262a35",
      "metadata": {
        "id": "f6262a35"
      },
      "outputs": [],
      "source": [
        "def select_initial_onset(onset_distribution):\n",
        "    ## STUDENT SECTION -\n",
        "    return np.random.choice()\n",
        "    ## END STUDENT SECTION ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168a9748",
      "metadata": {
        "id": "168a9748"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3;\">\n",
        "    \n",
        "- ðŸŽ¯ **Goal:** Complete the function *next_from_current_onset()*. The function should use *duration_distribution_by_onset* to access the duration distribution associated with *current_onset*, select a duration, and return **both** the duration and the next onset.\n",
        "\n",
        "- ðŸ’»  **API:** You can again use np.random.choice(). The \"add-on\" here is finding the proper distribution for application of np.random.choice(). Hint: the onset should fall in the range [0, 4). Accommodate a *next_onset* greater than 4 by performing the proper modulo operation.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5dc69b4",
      "metadata": {
        "id": "c5dc69b4"
      },
      "outputs": [],
      "source": [
        "def next_from_current_onset(current_onset, duration_distribution_by_onset):\n",
        "    ## STUDENT SECTION -\n",
        "    duration = np.random.choice(duration_distribution_by_onset[current_onset].index*4, p=duration_distribution_by_onset[current_onset])\n",
        "    next_onset = (current_onset + duration)%4\n",
        "    ## END STUDENT SECTION ##\n",
        "    return duration, next_onset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e4ac80",
      "metadata": {
        "id": "90e4ac80"
      },
      "source": [
        "Great! We'll pull your functions together to craft a generating method which takes sequence length as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6980a4",
      "metadata": {
        "id": "4c6980a4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "def rhythm_from_onset_dur_dists(length, onset_distribution, duration_distribution_by_onset):\n",
        "    onsets = []\n",
        "    durations = []\n",
        "    onsets.append(select_initial_onset(onset_distribution))\n",
        "    for o in range(length):\n",
        "        current_onset = onsets[-1]\n",
        "        duration, next_onset = next_from_current_onset(current_onset, duration_distribution_by_onset)\n",
        "        durations.append(duration)\n",
        "        onsets.append(next_onset)\n",
        "    onsets = [dur / 16 for dur in durations]\n",
        "    durations = [dur / 16 for dur in durations]\n",
        "    if onsets[0] != 0:\n",
        "        first_rest = [str(onsets[0])]\n",
        "        first_rest.extend(durations)\n",
        "        durations = first_rest\n",
        "    return helpers_rhythm_from_sequence(durations, time_sig='4/4')\n",
        "\n",
        "rhythm_from_onset_dur_dists(15, onset_distribution, duration_distribution_by_onset).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f83076",
      "metadata": {
        "id": "03f83076"
      },
      "source": [
        "\n",
        "Does this sound like Mozart? Well... no. Lack of pitch obviously affects this, but also, the memoryless nature of our generative tool means any motif or structure owes much to random number generation.\n",
        "\n",
        "This example simply provides one practical use-case for *helpers_rhythm_from_sequence()* and allows us to dip our toes into sampling from probability distributions - something we explore further in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0ee9a2",
      "metadata": {
        "id": "db0ee9a2"
      },
      "source": [
        "### ðŸŽ‰ Congrats! You have finished your introduction to rhythmic structure!\n",
        "\n",
        "###### [back to top](#0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}